---
layout: post
title: "Symmetries"
date: 2020-12-27
categories: machine-learning, symmetries
usemathjax: true
readtime: true
image: /assets/images/shell_web.jpg
subtitle: On symmetries.
excerpt_separator: <!--more-->
---

Symmetries.

* Contents
{:toc}

# Symmetries

* Models in the physical sciences may benefit from latent space representations which reflect the theory of the underlying symmetry group.
* Symmetries are manifest in the data as each data point is generated by a symmetric process or model.
* Restrict model architectures to represent functions which respect the underlying symmetries of the problem.
* This may improve model generalization, data/parameter efficiency, and model interpretability.
* Model parameters may be able to be directly interpreted in the context of known scientific models.

* A symmetry transformation leaves some aspect of the object invariant.
* Or can be a transformation of a description of an object, that gives an alternative description of the same underlying object.
* Consider symmetries of the label function $L: X \rightarrow Y$.
* $g: X \rightarrow X$ is a symmetry of $L$ if $L \cdot g = L$ ($L$ is invariant to the transform $g$).
* A transformation group is a set of transformations which:
  * Contains the identity transformation.
  * Is closed under composition (composition is associative).
  * Is closed under inverse (the inverse also exists).

## Orbits
* Group actions partition the input space into _orbits._
* The _orbit_ of a point $x$ is the set of all points that can be obtained by applying group transformations to the point $x$:
$$\begin{equation}
O_x = \{ g \cdot x \vert x \in X, g \in G \}
\end{equation}$$
* If $G$ is a symmetry of the learning problem, $L$ is the same for all elements in the orbit.

* Define an equivalence relation among input points $x_i,x_j$ are $G$-equivalent if $\exists \; g \in G$ s.t. $g \cdot x = y \implies x_i \sim_G x_j$.
  * Reflexity (identity)
  * Transitivity (closure under composition).
  * Symmetry (closure under inverse)

* Group acting on the input space partitions the space into orbits, all members of the orbit are $G$-equivalent.

## Equivariant Networks

* Translations are symmetries of many visual learning problems - where an object is located is often not important.
* Convolution layers exploit translation symmetry through weight sharing - same set of weights is applied at different positions. Detect same local visual patterns at different positions.
* Equivariance means the symmetry is preserved - later layers can exploit the symmetry as well.

### Group Representations
* In equivariant networks, the group acts on a different way for each feature space in the network.
* Consider linear group actions, GRs: a mapping from the group to the set of invertible $n \times n$ matrices, $GL(n)$.

$$\begin{equation}
    \rho: G \rightarrow GL(n), \quad \rho(g \cdot h) = \rho(g) \rho(h)
\end{equation}$$

i.e. representation of composition of group transforms is the same as matrix multiplication of the individual transformations of group elements. 

An equivariant network has:
* Feature vector spaces $X_i$.
* Mappings between these layers (feature spaces).
* A symmetry group $G$.
* Group representations $\rho_i$ of $G$ for each feature space $X_i$.

Equivariance means:

$$\begin{equation}
    f_i \cdot \rho_{i-1}(g) = \rho_i(g) \cdot f_i \; \forall \; i.
\end{equation}$$

i.e. The group transformation and application of the feature mapping commute for each feature space. Equivariance can be considered symmetry-consistent generalization. Imagine 2 input images $x,y$. Assume the mapping $\Phi$ maps $x,y$ to the same vector. Then the equivariant constraint says mapping $x$ to a new feature space and applying the symmetry transformation $\rho_i(g)$ in the new feature space is the same as mapping the symmetry-transformed input $\rho_{i-1}(g) \cdot x$ to the new feature space.

$$\begin{equation}
    \Phi \cdot \rho_{i-1}(g)(x) = \rho_i(g) \cdot \Phi(x).
\end{equation}$$

* The equivariant net must generalize consistently across the whole orbit - equivalent points are treated equivalently. If equivariance is built into the architecture, equivariance is guaranteed on unseen data.
* The architecture should be able to learn arbitrary equivariant maps between finite-dimensional representations of the symmetry group.
* One way of achieving this is through having activations that are elements of linear representations of the group.

## Convolutions
Planar convolution is one of the building blocks of deep nets - combines a number of inductive biases:
* Locality
* Spatial weight sharing
* Approximate translation equivariance.

The output feature map at $x \in \mathbb{Z}^2$ is computed as an inner product between the input feature map and a filter, shifted by $x$. Input is on $\mathbb{Z}^2$, filter is on $\mathbb{Z}^2$, output is on $\mathbb{Z}^2$.

### Spherical Convolution
Shifts are now rotations - the output feature map at $R \in SO(3)$ is computed as an inner product between the input feature map and a filter, rotated by $R$.

## Steerable Convolutions

How can we create a general convolution layer that will be equivariant with respect to the induced representation acting on the feature space?
Model convolution kernel as a map: $K: \mathbb{R}^2 \rightarrow \mathbb{R}^{C_{out} \times C_{in}}$. A _steerable convolution_ satisfies:

$$\begin{equation}
  K(r \cdot x) = \rho_{out}(r) K(x) \rho_{in}(r)^{-1}
\end{equation}$$

Rotating the input of the kernel is the same as taking the original kernel and 'steering' it using the group representations.

$$ f_{out}(x) = \int_{\mathbb{R}^2} dy \, K(y-x) f(y). $$

# Equivariant Networks

## Invariance vs. Equivariance
* Learning models have to account for the case where the same data appears in different forms. Let $\phi$ be the learning algorithm, then _invariance_ asserts that $\phi \cdot T_g(f) = \phi(f)$ - i.e. $f$ and it's transformed version are mapped to the same output.
* In contrast, equivariant networks map the transformed input to something which corresponds to the untransformed input in a coherent way. The group action and the operation of the learning algorithm are 'intertwined'. This can be a feature of the entire learning algorithm $\phi$ or only a subpart.

$$\begin{equation}
  \phi \cdot T_g^{(1)}(f) = T_g^{(2)} \cdot \phi(f)
\end{equation}$$

* If we want $\phi$ to be invariant, it can be advantageous to have components of $\phi$ be equivariant.

## Convolutional NNs
* Intepret as a pattern matching process - learn filters $g$ which are applied across the image-form input, across mutiple layers and channels.

$$\begin{equation}
  (f \ast g)(x) = \int dy \, f(x-y)g(y)
\end{equation}$$

* Translation of the input leads to translation of the output in the same way. Let $f'(x) = f(x-t)$ be the transformed function:

$$\begin{equation}
  (f' \ast g)(x) = \int dy \, f(x-t-y)g(y) = (f \ast g)(x-t).
\end{equation}$$

* Hierarchial structure coarsens the data as we progress up - critical for learning semantically meaningful features.

## Representations
* A mapping from a group to a set of linear operators, or matrices. The mapping is a homomorphism - the operator corresponding to the product of 2 group elements is the same as the product of the operators corresponding to the individual group elements:

$$\begin{equation}
  \rho(g_2g_1) = \rho(g_2)\rho(g_1)
\end{equation}$$

* Replaces abstract group elements with explicit matrices which are easier to reason about.
* Interested in looking at the action of the group elements on data.
* The compound linear transformation $T_{g_2 g_1}(v) = T_{g_2}T_{g_1}(v)$.
* Transformations which occur in learning algorithms boil down to analyzing group actions.

### Why linear actions?
* Some actions are linear to begin with, but some actions on a function space are linear by virtue of being induced on a base space.
  * e.g. consider functions on a sphere.
  * $G$ acts on a set $S$ by $x \leftarrow T_g(x)$.
  * $f: S \rightarrow \mathbb{R}$ is a function on $S$ and we want to learn $f \rightarrow h(f)$.
  * The induced action $T^i$ on the function space is $f \rightarrow f'$ where $f'(x) = f \cdot T_g^{-1} (x)$.
  * The transformed function $f'$ is the same as the composition of $f$ and the transform inverse $T_g^{-1}$ - the group action drags the function on $S$ to where it was on $S$. 
  * The induced action $T_g^i: L(s) \rightarrow L(s)$ is automatically linear.

### Equivalence/Reducibility
With 1 representation, there are many ways from which we can manufacture other representations
1. Two representations are said to be equivalent if: $$\rho'(g) = U\rho(g)U^{\dagger} $$
  * Conjugating with a unitary matrix leads to another which is regarded as the same.
1. A representation $\rho$ is said to be completely reducible if:

$$\begin{equation}
  \rho(g) = U  \left(
    \begin{array}{c|c}
      \rho_1(g) & 0\\
      \hline
      0 & \rho_2(g)
    \end{array}
    \right) U^{\dagger}
\end{equation}$$

  * For some fixed unitary matrix $U$. 
  * We can make more complicated representations from smaller ones by combining them in a block-diagonal form.
  * We can just study the individual parts of a representation which falls apart into block-diagonal form.

### Complete Reducibility
In the realm of compact groups, this process of taking representations apart into their constituent parts is unique.

**Theorem**: For a representation $\rho$ of a compact group $G$ on a vector space $V$, if $\rho$ fixes the subspace $W$, it also fixes the complementary subspace $W^{\perp}$.

$$\begin{equation}
  \rho(g) = U\left(
    \begin{array}{c|c}
      A(g) & B(g)\\
      \hline
      0 & C(g)
    \end{array}
    \right) U^{\dagger} \implies B(g) = 0
\end{equation}$$

**Corollary:** Any representation of a compact group is reducible into a direct sum of irreducible representations (irreps). Note irreps can always be chosen to be unitary: $\rho(g^{-1}) = \rho(g)^{\dagger}$ - inverse of group element is just the Hermitian conjugate.

### Schur's Lemma:
_(Representation spaces don't like to mix with each other.)_
Let $\phi: v \rightarrow w$ be a linear map between representation spaces. If $v$ transforms according to irrep $\rho$ of a compact group $G$ while $w$ transforms according to the irrep $\rho'$, then either:

1. $\rho = \rho'$ - the two irreps are the same and $\phi$ is a multiple of the identity map, or
2. $\phi = 0$.

## Designing Equivariant Neurons
Assume the inputs to some layer transform according to some representation of the underlying group symmetry, $f_i^{in} \rightarrow \rho_i^{in}(g) f_1^{in}$. We want the output to transform according to some representation of the group, $f^{out} \rightarrow \rho^{out}(g) f^{out}$.

A layer combines a linear transformation followed by a nonlinearity. The input transforms according to a representation of a group and can be broken up into vectors which transform according to irreps of the underlying group. We need a linear map which maps into it a preactivation which falls apart into the same combination of irreps. By Schur's lemma, vectors which transform according to different irreps are not allowed to mix with each other. Parts of the input move separately under different linear transformations. 

## Equivariant Nonlinearity
### Tensor Product Nonlinearity
Given two vectors $v_i \rightarrow \rho_i(g) v_i$, their tensor product transforms like:
$$\begin{equation}
  v_a \otimes v_b \rightarrow (\rho_a \otimes \rho_b)(g)(v_a \otimes v_b)
\end{equation}$$
* Quadratic nonlinearity, mixes isotypic channels, but in general not irreducible - how can we break it into irreducible parts?

### Clebsch-Gordan Decomposition - decomposing tensor products of irreps
The way the tensor product of two irreducible representations of a compact group reduces into a direct sum of irreps is given by the CG decomposition:

$$\begin{equation}
  \rho_a(g) \otimes \rho_b(g) = C_{\rho_a, \rho_b}\left[ \bigoplus_i \bigoplus_{j=1}^{\kappa(\rho_a, \rho_b, \rho_i)} \rho_i(g) \right] C_{\rho_a, \rho_b}^{\dagger}
\end{equation}$$

* $C_{\rho_a, \rho_b}$ is a fixed unitary matrix.
* Decomposes into the direct sum of different irreps - given by the inner sum, $\kappa$ is the multiplicity.

## Fourier Theory

Generalized convolution. Here $f$, $g$ are functions on the group $G$:

$$\begin{equation}
  (f \ast g)(x) = \int dy \, f(x-y)g(y) \rightarrow (f \ast g)(u) = \int_G d\mu(v) \, f(u v^{-1})g(v)
\end{equation}$$

* Here $d\mu(v)$ is the Haar measure - the analogue of the uniform measure on groups, well-defined for compact groups.

Group convolution is like pattern matching on the group.

### Generalized Fourier Transform

FT is integration of the function $f$ against a kernel on the real line.

$$\begin{equation}
  \hat{f}(k) = \int dx \, f(x) e^{-ikx} \rightarrow \hat{f}(\rho_i) = \int d\mu(x) f(x) \rho_i(x).
\end{equation}$$

Here $e^{-ikx}$ is the irrep of the translation group on the real line. The analogue for the convolution theorem is:

$$\begin{equation}
  \hat{f \ast g}(k) = \hat{f}(k) \cdot \hat{g}(k) = \hat{f \ast g}(\rho_i) = \hat{f}(\rho_i) \hat{g}(\rho_i)
\end{equation}$$


## References

<a id="1">[1]</a> 
Bingqing Cheng, Guglielmo Mazzola, Chris J. Pickard, Michele Ceriotti. (2020) Evidence for supercritical behaviour of high-pressure liquid hydrogen. Nature, 585, 217–220.

<a id="2">[2]</a> 
Bingqing Cheng, Edgar A Engel, Jörg Behler, Christoph Dellago, Michele Ceriotti. (2019) ab initio thermodynamics of liquid and solid water. Proceedings of the National Academy of Sciences, 116 (4), 1110-1115.
